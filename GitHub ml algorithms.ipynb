{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Uploading Datasets <a name=\"introduction\"></a>\n",
    "Uploading given documents \"spambase.names\" and \"spambase.data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spambase.names') as f:\n",
    "    list_contents = f.readlines()\n",
    "    list_colnames = []\n",
    "    for item in list_contents:\n",
    "        colname = item.split(':')[0]\n",
    "        list_colnames.append(colname)\n",
    "list_colnames.append('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('spambase.data', header=None)\n",
    "df_dataset.columns = list_colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis-EDA <a name=\"DataExploration\"></a>\n",
    "\n",
    "Anlyzing dataframe to summarize main feauters, data visualizations, statistical graphics. The goal is create better uderstanding of our data and create insights about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(kind='count', \n",
    "           data=df_dataset,\n",
    "           x='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of training models with unbalancing data, model can show serious biases. The main thing we trying to do in our model is understand what inputs affect the label (spam or not). The model trained with more spam data will more likely to give spam output. Balancing can be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='label', y='word_freq_make', data = df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='label', y='word_freq_money', data = df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='label', y='word_freq_mail', data = df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='label', y='word_freq_meeting', data = df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Correlation\n",
    "\n",
    "Correlation is very important metric to understand the level of relation between features in the dataset. As we can see from the \"correlation\" function, 'word_freq_415' feature shows high correlation with dataset. High correlated features nearly have the same chance to predict output. Removing on of the correlated variable can increase learning process; decrease the complexity of the alghoritm and error risks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)-1):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                #next, decide which feature to drop. We compare the feature corr with target\n",
    "                if abs(corr_matrix.iloc[j, len(corr_matrix.columns)-1]) > abs(corr_matrix.iloc[i, len(corr_matrix.columns)-1]):\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                else:\n",
    "                    colname = corr_matrix.columns[j]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = correlation(df_dataset, 0.90)\n",
    "print(corr_features)\n",
    "print('number of corr features: '+str(len(set(corr_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60, 50))\n",
    "ax=sb.heatmap(df_dataset.corr(),linewidths=1,annot=True, fmt=\".2\",cmap=sb.cm.rocket_r)\n",
    "ax.tick_params(axis='x', labelrotation=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the given features were highly correlated with other variables. There was only one feature correlated but in this tast we decieded to not to include model testing with less numbers of feautures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feauter scaling techniqeu StandartScaler used to scale all feauters around the mean with standart deviation. Some of the feauters can be sclaed different others. This techniqeu creat a consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_inputs = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering and Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the process of independent and dependent variables defined. None of the given features were highly correlated with other variables. This is why \"drop\", \"delete\", \"pop\" didn't used in this task.  \n",
    "\n",
    "x (independent) variable going to be our \"Input\" and y (dependent) variable going to be our \"Output\". Dependent variable descibe mail spam or not spam with binary method. Independent variables are frequency of words used in the mail.  \n",
    "\n",
    "The goal is creating models describes the relationship between inputs and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_dataset.drop(['label'],axis=1)\n",
    "y=df_dataset['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split Train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason splitting a dataset into training and testing subsets is unbiasedly validate how accurate the model is. It is not really efficient to score the model with the data model alredy trained. The the data will be splitted before training of the model. Then in the prediction and validation processes, test data will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test size define what percentage of dataset we wanna split for testing. Randomization during splitting defined as \"random_state\". We also splited a different train-test data set with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train- test split with scaled dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Unscaled dataset\n",
    "x_train_unscaled, x_test_unscaled, y_train_unscaled, y_test_unscaled = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests is supervised machine learning method for classification (as it is in our problem), regression and other tasks. In classification problems, the output of the classifier is selected by majority of the trees.  \n",
    "\n",
    "The goal is comparison of the default parameters vs grid search. The change on accuracy score should show us the importance of the grid search in case of getting the best possible solution. After than, accuracy of Random Forest will be compared with other ML models.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   A. Random Forest- Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators.: It is coming from 'how many trees in the forest'.  \n",
    "bootstrap.: If it is \"False\" it shows that all data set used otherwise it sampled. \"true\" is default.  \n",
    "max_features.: Max feauters given to the each tree. \"sqrt\" is the default feature.  \n",
    "min_sample_split.: At least, how many samples/observations are needed to separate any node. \"2\" is default.  \n",
    "min_samples_leaf.: After splitting, at least how many samples needed in leaf node. \"1\" is default min sample size.  \n",
    "max_depth.: Maximum lenght of tree in the forest. If it is \"none\" which is default, it grows until all   leaves contain less than min_samples_split samples.  \n",
    "\n",
    "max_leaf_nodes.: It is about splitting nodes in the tree and arrange separately growth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "parameters_rf = {\n",
    "#    'n_estimators': [25, 50, 100, 500],\n",
    "#    'bootstrap': [True, False],\n",
    "#    'max_features': [\"log2\", \"auto\",\"sqrt\"],\n",
    "#    'min_samples_split': range(2,10,1),    \n",
    "#    'min_samples_leaf': range(1,10,1),\n",
    "#    'max_depth': range(2,20,1),\n",
    "    \n",
    "    'n_estimators': [25, 50, 100],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': [\"log2\", \"auto\",],\n",
    "    'min_samples_split': range(2,4,1),    \n",
    "    'min_samples_leaf': range(1,4,1),\n",
    "    'max_depth': range(2,200,10),\n",
    "             }\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator = rf, \n",
    "                           param_grid = parameters_rf, \n",
    "                           cv = 5, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 3)\n",
    "\n",
    "model_rf=grid_search_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scoring process, using test data which is model never seen before, is highly important to reach unbiased scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.score(x_train, y_train))\n",
    "print(grid_search_rf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_=grid_search_rf.predict(x_test)\n",
    "grid_search_rf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B. Random Forest Grid Search vs Search With Default Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_random_forest= RandomForestClassifier().fit(x_train,y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest Accuracy Score: {:.2f}'.format(accuracy_score(y_test, model_rf_)))\n",
    "print('Random Random Forest Accuracy Score: {:.2f}'.format(accuracy_score(y_test, random_random_forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing the results of Random Forest with default parameters and Grid Search, we can say that Grid Search helped to showed slightly better result than default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Classifier is an \"ensemble method\" which creats new sample dataset with replacement. It can cause repeated datas or not including some datas. Voting and averaging in Bagging Classifier, reduce overfitting which cause biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_svc = BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=None)\n",
    "model_svc=model_svc.fit(x_train, y_train)\n",
    "model_svc_=model_svc.predict(x_test)\n",
    "model_svc.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a gradient-boosting library that has been developed to be very effective, adaptable, and portable. It uses the Gradient Boosting framework to implement machine learning algorithms. A parallel tree boosting method called XGBoost is available to quickly and accurately address a variety of data science issues. The same algorithm can answer problems with more than a trillion examples and runs on key distributed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb= GradientBoostingClassifier(n_estimators=120, learning_rate=1,max_depth=1, random_state=0)\n",
    "model_gb=model_gb.fit(x_train, y_train)\n",
    "model_gb_ = model_gb.predict(x_test)\n",
    "\n",
    "model_gb.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Adaboost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost Classifier is an \"ensemble method\" which use boosting strategy. In random forest default estimator is 'decision tree classifier' with 1 max depth.\n",
    "\n",
    "n_estimators.: In boosting max number of estimators. For the best fit learning stop at pick point. Default= 50  \n",
    "learning rate.: In each itteration the wight on classifier. Default= 1.  \n",
    "random state.: It controls the randomness in each itteration at each estimator. Default= None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. AdaBoostClassifier Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoostClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada= AdaBoostClassifier()\n",
    "   \n",
    "parameters_ada= {\n",
    "    'n_estimators':[100, 250, 500, 600, 750, 1000],\n",
    "    'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator=model_ada, param_grid=parameters_ada, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "model_ada=grid_search_ada.fit(x_train, y_train)\n",
    "model_ada_= model_ada.predict(x_test)\n",
    "\n",
    "print(grid_search_ada.best_params_)\n",
    "print(model_ada.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. AdaBoost Hyper Parameter Tuning vs Default Parameters Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forest default estimator is 'decision tree classifier' with 1 max depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ada_model= AdaBoostClassifier().fit(x_train, y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ada Boost Accuracy Score: {:.2f}'.format(accuracy_score(y_test, model_ada_)))\n",
    "print('Default Ada Boost Accuracy Score: {:.2f}'.format(accuracy_score(y_test, random_ada_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "    \n",
    "As a conclusion, in this problem grid search in AdaBoost Classifier gived better results than default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Adaboos Estimators Grid Searchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"random_ada_model\" saved as base model\n",
    "ada_dtree = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "ada_extr = AdaBoostClassifier(ExtraTreeClassifier())\n",
    "ada_logistic = AdaBoostClassifier(LogisticRegression())\n",
    "ada_svc = AdaBoostClassifier(SVC(probability=True , kernel='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = {'base_estimator__max_depth' :[1, 2, 5],\n",
    "              'base_estimator__min_samples_split' :[2, 3 ,5],\n",
    "              'base_estimator__min_samples_leaf' :[2, 3, 5 ,10],\n",
    "              'n_estimators' :[600, 750],\n",
    "              'learning_rate' :[0.01, 0.1, 1]}\n",
    "\n",
    "\n",
    "ada_dtree = GridSearchCV(estimator=ada_dtree,\n",
    "                        param_grid=parameters,\n",
    "                        scoring='roc_auc',\n",
    "                        return_train_score=True,)\n",
    "\n",
    "ada_dtree.fit(x_train, y_train)\n",
    "ada_dtree_=ada_dtree.predict(x_test)\n",
    "\n",
    "ada_dtree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'base_estimator__criterion' :['gini', 'entropy'],\n",
    "              'base_estimator__max_depth' :[1, 2, 5],\n",
    "              'base_estimator__min_samples_split' :[2, 3 ,5],\n",
    "              'base_estimator__min_samples_leaf' :[2, 3, 5 ,10],\n",
    "              'n_estimators' :[300, 600],\n",
    "              'learning_rate' :[0.1, 0.01]}\n",
    "\n",
    "\n",
    "\n",
    "ada_extr = GridSearchCV(estimator=ada_extr,\n",
    "                        param_grid=parameters ,\n",
    "                        scoring='roc_auc',\n",
    "                        return_train_score=True,\n",
    "                       )\n",
    "\n",
    "ada_extr= ada_extr.fit(x_train, y_train)\n",
    "ada_extr_=ada_extr.predict(x_test)\n",
    "\n",
    "ada_extr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'base_estimator__C' :[0.01, 0.1, 1, 10, 50, 100, 500],\n",
    "              'n_estimators' :[100, 250, 500, 750],\n",
    "              'learning_rate' :[0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "\n",
    "ada_logistic= GridSearchCV(estimator=ada_logistic,\n",
    "                        param_grid=parameters,\n",
    "                        scoring='roc_auc',\n",
    "                        return_train_score=True,\n",
    "                          )\n",
    "\n",
    "ada_logistic= logr_grid.fit(x_train, y_train)\n",
    "ada_logistic_=logr_grid.predict(x_test)\n",
    "\n",
    "ada_logistic.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'base_estimator__C' :[0.01, 0.1, 1, 10, 50, 100, 500, 1000],\n",
    "              'n_estimators' :[10, 50, 100, 250, 500, 750, 1000],\n",
    "              'learning_rate' :[0.0001, 0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "\n",
    "ada_svc = GridSearchCV(estimator=ada_svc,\n",
    "                        param_grid=parameters,\n",
    "                        scoring='roc_auc',\n",
    "                        return_train_score=True,\n",
    "                        )\n",
    "\n",
    "ada_svc= ada_svc.fit(x_train, y_train)\n",
    "ada_svc_=ada_svc.predict(x_test)\n",
    "\n",
    "ada_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_scores_all= [accuracy_score(y_test, random_ada_model), accuracy_score(y_test, ada_dtree_),\n",
    "                  accuracy_score(y_test, ada_extr_), accuracy_score(y_test, ada_logistic_),\n",
    "                 accuracy_score(y_test, ada_svc_)]\n",
    "\n",
    "name_ada_all= ['basic_ada_model','ada_dtree','ada_extr','ada_logistic','ada_svc']\n",
    "\n",
    "pd.DataFrame(ada_scores_all, name_ada_all, \"accuracy score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier has collective judgment methodology to predict data points. It uses multiple models.  \n",
    "\n",
    "estimators: models will be used for voting classifier model.  \n",
    "voting: It can be hard or soft. Hard use predicted class labels but soft predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_clf = VotingClassifier(\n",
    "    estimators = [('rndf', model_rf), ('svc', model_svc), ('ada', model_ada)],\n",
    "    voting = 'soft')\n",
    "\n",
    "vot_clf = vot_clf.fit(x_train, y_train)\n",
    "vot_clf_=vot_clf.predict(x_test)\n",
    "vot_clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Hyperparameter Tuning- ANN\n",
    "\n",
    "Gridsearch through two for loop on \"batch size\" and \"list epoch\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "list_batch_size=[10,25,32]\n",
    "list_epoch=[50,75,110]\n",
    "\n",
    "GSResultsData=pd.DataFrame(columns=['counter', 'Parameter Results', 'Accuracy Score'])\n",
    "\n",
    "for trial_batch_size in list_batch_size:\n",
    "        for trial_epochs in list_epoch:\n",
    "            counter+=1\n",
    "            \n",
    "            model = keras.models.Sequential([\n",
    "                keras.layers.Dense(100, activation='relu', input_dim=57),\n",
    "                keras.layers.Dense(66, activation='relu'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            model.compile(optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "                              loss='binary_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "                    \n",
    "            history = model.fit(x_train, y_train, batch_size=trial_batch_size, epochs=trial_epochs,\n",
    "                                validation_split=.1,callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "            \n",
    "            Accuracy = history.history['accuracy'][-1]\n",
    "            \n",
    "            print(counter, 'Parameter Results','batch_size:', trial_batch_size,'-', 'epochs:',trial_epochs,\n",
    "                  'Accuracy Score:', Accuracy)\n",
    "            \n",
    "    \n",
    "            #plt.plot(history.history['accuracy'])\n",
    "            #plt.plot(history.history['val_accuracy'])\n",
    "            #plt.title('accuracy model')\n",
    "            #plt.ylabel('accuracy')\n",
    "            #plt.xlabel('epoch')\n",
    "            #plt.legend(['training', 'validation'], loc='best')\n",
    "            #plt.show()\n",
    "            \n",
    "            GSResultsData=GSResultsData.append(pd.DataFrame\n",
    "                                                       (data=[[counter,'batch_size'+\n",
    "                                                               str(trial_batch_size)+'-'+'epoch'+\n",
    "                                                               str(trial_epochs), Accuracy]],\n",
    "                                                                    columns=[\n",
    "                                                                        'TrialNumber', 'Parameters', 'Accuracy'] ))\n",
    "            GSResultsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 The Best Hyperparameter Result\n",
    "\n",
    "In the grid search, accuracy score dependent on batch size and epoch size plotted. Best parameters will be used on ann model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GSResultsData.sort_values(by='Accuracy', ascending=False).head(1))\n",
    "GSResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line', rot=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B- ANN Model Training\n",
    "\n",
    "Training the model with best batch size and epoch given by grid search. There are 3 hidden layers with 'relu', 'relu','sigmoid' activation functions. There is 1 input and output layer.\n",
    "\n",
    "  Activation function makes transformation on variables. Otherwise, a neural network model would be just a linear regression. If we gonna use linear regression, it won be strong enough to model data efficiently. Number of layers wont be meaningful and model will be like singel layer model.  \n",
    "  \n",
    "  In \"Binary Classifier\", sigmoid function shoul be used, in the last layer with one node. Because, output need to transform 0-1 binary form. Sigmoid will transform values, dependent on a probability value. It is not common to use sigmoid function in hidden layers because of the drawbacks like 'saturation of the gradients problem\",slow convergence etc.  \n",
    "  \n",
    "  In the 'multiclass classification', softmax activation function in per class one node.  \n",
    "  In the 'multilabel classification', sigmoid activation function in per class one node.  \n",
    "  In \"Hidden Layers\" non linear functions shoul be used as the activation function. ReLU is the most populer one and if results not sattisfying, leaky ReLU is an option.\n",
    "  \n",
    "  Functions to only use in hidden layers: ReLU, variants of ReLU, Tanh, swish and hard swish functions.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(100, activation='relu', input_dim=57),\n",
    "        keras.layers.Dense(66, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of \"early stoppig\" is avoid overfitting of the training dataset. It can also cause underfitting, so implementation of this process is very important.\n",
    "\n",
    "monitor.: Monitors the performance, 'loss function' during training. \n",
    "patience.: How many epochs later training will be stoped if there is no improvement.  \n",
    "min_delta.: It is a parameter define min number of change to qualify as improvement. If it is less than \"min_delta\", it wont qualify as improvement.  \n",
    "mode.: There are 3 settings; auto, min, max. Auto make automatic inferred for monitored value. Min will stopped when monitored value stop decreasing. Max will stopped when monitored value stop increasing.  \n",
    "\n",
    "As we can see from \"C. ANN Model With Standartized Dataset\" part, early stopping work well. Avoiding overfitting by stoping training early.\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "If the learning rate is too low, it makes many updates untill the min point.\n",
    "If the learning rate is too high, it make updates with big steps and leads divergent behaviour.\n",
    "If the learning rate is just right,it quickly reach the optimal point.\n",
    "\n",
    "Model trained with '0.005' and '0.001' learning rate and the results shows that '0.001'gives better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_train, output_train):\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(input_train, output_train, batch_size=10, epochs=110, validation_split=.1,\n",
    "                       callbacks=[keras.callbacks.EarlyStopping(\n",
    "                           monitor='val_accuracy', \n",
    "                           patience=10, \n",
    "                           min_delta=0.001, \n",
    "                           mode='max')])\n",
    "\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('accuracy model')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= create_network()\n",
    "model2=evaluate(model, x_train, y_train)\n",
    "model3= model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn to convert probability to a class\n",
    "model_ann=[]\n",
    "for variable in model3:  \n",
    "    \n",
    "    if variable<0.5:\n",
    "        model_ann.append(0)\n",
    "    else:\n",
    "        model_ann.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. ANN Model With Unscaled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unscaled= create_network()\n",
    "model_evaluated_unscaled=evaluate(model_unscaled, x_train_unscaled, y_train_unscaled)\n",
    "model_unscaled_predicted= model_evaluated_unscaled.predict(x_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the accuracy model plot, training accuracy score keep increasing. However, validation score already stop increasing early stages of the training. It could cause overfitting in case early stopping havent been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann_unscaled=[]\n",
    "for variable in model_unscaled_predicted:  \n",
    "    if variable<0.5:\n",
    "        model_ann_unscaled.append(0)\n",
    "    else:\n",
    "        model_ann_unscaled.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D- ANN Scaled and Unscaled Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('ANN model Accuracy Score: {:.2f}\\n'.format(accuracy_score(y_test, model_ann)))\n",
    "print('ANN model unsclaed Accuracy Score: {:.2f}'.format(accuracy_score(y_test_unscaled, model_ann_unscaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results, model with scaled dataset shows better accuracy than model with unscaled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is one of the performance metrics for classification algorithm. There are 4 metrics \"tp\", \"tn\", \"fp\", \"fn\". True Positives and True Negatives are the ones we would like to see more than False Positives and False Negatives. False Positives and False Negative are types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for clf in (model_rf_, model_svc_, model_ada_, vot_clf_, model_gb_, model_ann):\n",
    "\n",
    "    name=clf.__class__.__name__\n",
    "    confusion=confusion_matrix(y_test, clf)\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion, index = [\"true\",'false'],\n",
    "                  columns = [\"true\",'false'])\n",
    "    plt.figure(figsize = (3,3))\n",
    "    sb.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Cross Validation Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the sklearn documentation linked below, for classification problems there are multiple validation scorings. Cross Validation is one of them. It is a method can be employed on classification, clustring, regression problems to estimate overall accuracy. Specifically for classification problems for cross validation score there are metrics such as ‘f1’, 'precision’, ‘recall’. All metrics shows a score between 0 and 1. Results are accepted better when getting close to 1.\n",
    "\n",
    "To calculate F1 score we use both precision and recall (F1 = 2 * (precision * recall) / (precision + recall).  \n",
    "To calculate Precision score we use formulation with both \"true positives\" and \"false positives\" (tp / (tp + fp)). How many of positive predicted ones are really postives.  \n",
    "To calculate Recall score we use formulation with both \"true positives\" and \"false negatives\" tp / (tp + fn). From all positives, how many predicted correctly.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (model_rf_, model_svc_, model_ada_, vot_clf_, model_gb_, model_ann):\n",
    "    name=clf.__class__.__name__\n",
    "    #names=[\"model_rf_\",\" model_svc_\", \"model_ada_\", \"vot_clf_\"]\n",
    "    print(\"\\n\",name)\n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, clf)))\n",
    "\n",
    "    print('Micro Precision: {:.2f}'.format(precision_score(y_test, clf, average='micro')))\n",
    "    print('Macro Precision: {:.2f}'.format(precision_score(y_test, clf, average='macro')))\n",
    "    print('Weighted Precision: {:.2f}\\n'.format(precision_score(y_test, clf, average='weighted')))\n",
    "    \n",
    "    print('Micro Recall: {:.2f}'.format(recall_score(y_test, clf, average='micro')))\n",
    "    print('Macro Recall: {:.2f}'.format(recall_score(y_test, clf, average='macro')))\n",
    "    print('Weighted Recall: {:.2f}\\n'.format(recall_score(y_test, clf, average='weighted')))\n",
    "    \n",
    "    print('Micro F1-score: {:.2f}'.format(f1_score(y_test, clf, average='micro')))\n",
    "    print('Macro F1-score: {:.2f}'.format(f1_score(y_test, clf, average='macro')))  \n",
    "    print('Weighted F1-score: {:.2f}\\n'.format(f1_score(y_test, clf, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCLASSIFICATION REPORT\\n\\n')\n",
    "for clf in (model_rf_, model_svc_, model_ada_, vot_clf_, model_gb_, model_ann):\n",
    "    \n",
    "    name=clf.__class__.__name__\n",
    "    print(name,classification_report(y_test, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#class_name=[]\n",
    "score_precision=[]\n",
    "score_f1=[]\n",
    "score_recall=[]\n",
    "score_accuracy=[]\n",
    "for clf in (model_rf_, model_svc_, model_ada_, vot_clf_, model_gb_, model_ann):\n",
    "\n",
    "    score_precision.append(precision_score(y_test, clf, average='weighted'))\n",
    "    score_f1.append(f1_score(y_test, clf, average='weighted'))\n",
    "    score_recall.append(recall_score(y_test, clf, average='weighted'))\n",
    "    score_accuracy.append(accuracy_score(y_test, clf))\n",
    "    #class_name.append(clf.__class__.__name__)\n",
    "    class_name=[\"random forest\",\"SVC\",\"adaboost\",\"vooting\",\"gradient boosting\",\"ANN\"]\n",
    "    \n",
    "df = pd.DataFrame(list(zip(class_name,score_precision, score_f1, score_recall ,score_accuracy)),\n",
    "               columns =['model', 'precision','f1','recall','accuracy'])\n",
    "\n",
    "df.sort_values(by=['precision'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data frame, there was 57 word frequency column and 1 label column as binary classification which defines spam (1) or not spam (0). After EDA  it was found that the label was not balanced. After correlation analyses, it was seen that one of the variables was correlated with another variable. Variables were in different scales, so preprocessing \"standardization\" was applied to the data frame.  \n",
    "\n",
    "In the random forest we did \"Hyper Parameter Tuning\" and compared results with default parameters. Results showed that hyperparameter tuning changed the 0.95 accuracy score to 0.96 which made random forest the best model in the validation scoring part between all models. In ANN hyperparameter tuning 10 batch size, 110 epoch parameters showed better accuracy. After the ANN model was trained with both scaled and unscaled datasets with found parameters as the result scaled dataset gave a \"0.03\" better solution than the unscaled dataset. Early stopping is used to stop training when it shows no improvement in the validation set. In the ANN part, early stopping is applied when there is no improvement in 10 epochs (patient). After model training results show that hyperparameter tuning and standardization increased model accuracy.  \n",
    "\n",
    "In the validation part, the difference between the actual value and the predicted value is visualized with a confusion matrix. Then cross-validation metrics such as accuracy, f1, precision, and recall are applied to the model. In conclusion, the random forest gave the best results with a 0.96 accuracy score. After the random forest, gradient boosting and ANN gave good scores with 0.95 accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
